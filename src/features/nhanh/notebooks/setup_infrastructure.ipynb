{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Nhanh Feature Infrastructure Setup\n",
                "\n",
                "This notebook helps manage the infrastructure setup for the **Nhanh** feature.\n",
                "It consolidates creating secrets, BigQuery tables, Cloud Run Jobs (Core ETL & Transform), Cloud Scheduler jobs, and Monitoring alerts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import getpass\n",
                "import subprocess\n",
                "from google.cloud import bigquery\n",
                "\n",
                "# Configuration Variables\n",
                "PROJECT_ID = \"sync-nhanhvn-project\"\n",
                "REGION = \"asia-southeast1\" \n",
                "REGION_SCHEDULER = \"us-central1\"\n",
                "REGION_RUN = \"asia-southeast1\"\n",
                "SERVICE_ACCOUNT = f\"{PROJECT_ID}@appspot.gserviceaccount.com\"\n",
                "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/nhanh-etl:latest\"\n",
                "\n",
                "print(f\"Project: {PROJECT_ID}\")\n",
                "print(f\"Service Account: {SERVICE_ACCOUNT}\")\n",
                "print(f\"Image: {IMAGE_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Secrets\n",
                "Securely adding Nhanh credentials to Google Secret Manager."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Please provide Nhanh credentials (press Enter to skip if already set):\")\n",
                "nhanh_app_id = getpass.getpass(\"Nhanh App ID: \")\n",
                "nhanh_business_id = getpass.getpass(\"Nhanh Business ID: \")\n",
                "nhanh_access_token = getpass.getpass(\"Nhanh Access Token: \")\n",
                "\n",
                "def create_secret(name, value):\n",
                "    if not value: return\n",
                "    try:\n",
                "        subprocess.run(f\"gcloud secrets describe {name} --project={PROJECT_ID}\", shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
                "        print(f\"Secret '{name}' exists. Adding new version...\")\n",
                "        cmd = f\"gcloud secrets versions add {name} --data-file=- --project={PROJECT_ID}\"\n",
                "    except subprocess.CalledProcessError:\n",
                "        print(f\"Creating secret '{name}'...\")\n",
                "        cmd = f\"gcloud secrets create {name} --data-file=- --project={PROJECT_ID}\"\n",
                "    \n",
                "    process = subprocess.run(cmd, input=value.encode(), shell=True, capture_output=True)\n",
                "    if process.returncode == 0:\n",
                "        print(f\"Success: {name}\")\n",
                "    else:\n",
                "        print(f\"Error: {process.stderr.decode()}\")\n",
                "\n",
                "    subprocess.run(f\"gcloud secrets add-iam-policy-binding {name} --member='serviceAccount:{SERVICE_ACCOUNT}' --role='roles/secretmanager.secretAccessor' --project={PROJECT_ID}\", shell=True)\n",
                "\n",
                "create_secret(\"nhanh-app-id\", nhanh_app_id)\n",
                "create_secret(\"nhanh-business-id\", nhanh_business_id)\n",
                "create_secret(\"nhanh-access-token\", nhanh_access_token)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Bronze Tables (BigQuery)\n",
                "Creates external tables for Nhanh data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = bigquery.Client(project=PROJECT_ID)\n",
                "\n",
                "tables = {\n",
                "    \"nhanh_bills_raw\": \"gs://{PROJECT_ID}-bronze/nhanh/bills/**/*.json.gz\",\n",
                "    \"nhanh_bill_products_raw\": \"gs://{PROJECT_ID}-bronze/nhanh/bill_products/**/*.json.gz\",\n",
                "    \"nhanh_products_raw\": \"gs://{PROJECT_ID}-bronze/nhanh/products/**/*.json.gz\",\n",
                "    \"nhanh_customers_raw\": \"gs://{PROJECT_ID}-bronze/nhanh/customers/**/*.json.gz\"\n",
                "}\n",
                "\n",
                "for table_name, uri_pattern in tables.items():\n",
                "    uri = uri_pattern.format(PROJECT_ID=PROJECT_ID)\n",
                "    sql = f\"\"\"\n",
                "    CREATE OR REPLACE EXTERNAL TABLE `{PROJECT_ID}.bronze.{table_name}`\n",
                "    OPTIONS (\n",
                "      format = 'JSON',\n",
                "      uris = ['{uri}'],\n",
                "      compression = 'GZIP'\n",
                "    );\n",
                "    \"\"\"\n",
                "    print(f\"Creating {table_name}...\")\n",
                "    client.query(sql).result()\n",
                "print(\"All tables created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Setup Core ETL Jobs\n",
                "Creates/Updates Cloud Run Jobs for Bronze, Silver, and Gold layers.\n",
                "These jobs use the same image but are scheduled differently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Common Environment Variables\n",
                "ENV_VARS = f\"GCP_PROJECT={PROJECT_ID},GCP_REGION={REGION_RUN},BRONZE_BUCKET={PROJECT_ID}-bronze,SILVER_BUCKET={PROJECT_ID}-silver,BRONZE_DATASET=bronze,SILVER_DATASET=silver,GOLD_DATASET=gold,PARTITION_STRATEGY=month,LOG_LEVEL=INFO\"\n",
                "SECRETS = \"NHANH_APP_ID=nhanh-app-id:latest,NHANH_BUSINESS_ID=nhanh-business-id:latest,NHANH_ACCESS_TOKEN=nhanh-access-token:latest\"\n",
                "\n",
                "jobs = [\n",
                "    \"nhanh-etl-job\",          # Bronze Extraction\n",
                "    \"nhanh-etl-silver-job\",   # Silver Transformation\n",
                "    \"nhanh-etl-gold-job\"      # Gold Aggregation\n",
                "]\n",
                "\n",
                "for job in jobs:\n",
                "    print(f\"Processing {job}...\")\n",
                "    cmd = f\"\"\"gcloud run jobs create {job} \\\n",
                "      --image={IMAGE_NAME} \\\n",
                "      --region={REGION_RUN} \\\n",
                "      --service-account={SERVICE_ACCOUNT} \\\n",
                "      --memory=2Gi \\\n",
                "      --cpu=2 \\\n",
                "      --max-retries=3 \\\n",
                "      --task-timeout=3600 \\\n",
                "      --set-env-vars=\"{ENV_VARS}\" \\\n",
                "      --set-secrets=\"{SECRETS}\"\"\"\"\n",
                "    \n",
                "    # Try create, if fails (already exists), assume update (or use specific update logic)\n",
                "    res = subprocess.run(cmd, shell=True, capture_output=True)\n",
                "    if res.returncode != 0:\n",
                "        if \"already exists\" in res.stderr.decode():\n",
                "            print(f\"{job} exists, updating...\")\n",
                "            cmd_update = f\"\"\"gcloud run jobs update {job} \\\n",
                "              --image={IMAGE_NAME} \\\n",
                "              --region={REGION_RUN} \\\n",
                "              --set-env-vars=\"{ENV_VARS}\" \\\n",
                "              --set-secrets=\"{SECRETS}\"\"\"\"\n",
                "            subprocess.run(cmd_update, shell=True)\n",
                "        else:\n",
                "            print(f\"Error creating {job}: {res.stderr.decode()}\")\n",
                "    else:\n",
                "        print(f\"Created {job}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Setup Cloud Scheduler\n",
                "Sets up the schedulers for Bronze, Silver, Gold layers and Daily Syncs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "schedulers = [\n",
                "    # Bronze: Every 30 mins\n",
                "    f\"gcloud scheduler jobs create http nhanh-etl-bronze-schedule --location={REGION_SCHEDULER} --schedule='*/30 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--entity\\\",\\\"all\\\",\\\"--incremental\\\"]}}'\",\n",
                "    # Silver: Every 45 mins (offset 15)\n",
                "    f\"gcloud scheduler jobs create http nhanh-etl-silver-schedule --location={REGION_SCHEDULER} --schedule='15,45 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-silver-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--entity\\\",\\\"all\\\"]}}'\",\n",
                "    # Gold: Every hour (offset 30)\n",
                "    f\"gcloud scheduler jobs create http nhanh-etl-gold-schedule --location={REGION_SCHEDULER} --schedule='30 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-gold-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--aggregate\\\",\\\"all\\\"]}}'\",\n",
                "    # Bills Daily Sync\n",
                "    f\"gcloud scheduler jobs create http etl-api-bigquery-nhanh-bills-daily-sync-schedule --location={REGION_SCHEDULER} --schedule='0 1 * * *' --uri='https://{REGION_RUN}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/etl-api-bigquery-nhanh-bills-daily-sync:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID}\"\n",
                "]\n",
                "\n",
                "for cmd in schedulers:\n",
                "    print(f\"Setting up scheduler... {cmd.split(' ')[4]} \")\n",
                "    subprocess.run(cmd + \" 2>nul || echo 'Job likely exists'\", shell=True)\n",
                "    print(\"Done.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Setup Bills Transform Pipeline (Eventarc)\n",
                "Triggers bills transformation on GCS uploads."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BUCKET_NAME = PROJECT_ID\n",
                "TRANSFORM_JOB_NAME = \"nhanh-bills-transform-job\"\n",
                "TRIGGER_NAME = \"gcs-trigger-bills-transform\"\n",
                "\n",
                "# See previous cell logic for job creation/update if needed, or assume handled by CI/CD for this specific job\n",
                "# But for completeness let's ensure it exists via gcloud\n",
                "\n",
                "# Create/Update Bills Transform Job\n",
                "# ... (logic to create/update nhanh-bills-transform-job, skipping for brevity as previously defined)\n",
                "\n",
                "# Setup Eventarc\n",
                "print(\"Setting up Eventarc trigger...\")\n",
                "cmd_trigger = f\"gcloud eventarc triggers create {TRIGGER_NAME} --location={REGION_RUN} --destination-run-job={TRANSFORM_JOB_NAME} --destination-run-job-region={REGION_RUN} --destination-run-job-task-timeout=1800 --event-filters='type=google.cloud.storage.object.v1.finalized' --event-filters='bucket={BUCKET_NAME}' --event-filters='objectNamePrefix=nhanh/' --event-filters='objectNameSuffix=.parquet' --service-account={SERVICE_ACCOUNT} --project={PROJECT_ID}\"\n",
                "subprocess.run(f\"gcloud eventarc triggers delete {TRIGGER_NAME} --location={REGION_RUN} --project={PROJECT_ID} --quiet\", shell=True, stderr=subprocess.DEVNULL)\n",
                "subprocess.run(cmd_trigger, shell=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Setup Alerts\n",
                "Creates monitoring policies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHANNEL_ID = \"projects/{PROJECT_ID}/notificationChannels/YOUR_CHANNEL_ID_HERE\"\n",
                "print(\"Ensure you replace YOUR_CHANNEL_ID_HERE with a valid Notification Channel ID.\")\n",
                "\n",
                "# ... (Alert creation commands)"
            ]
        }\n"," ]\n",
        "}"
    ]
},
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Scheduler commands\n",
        "\n",
        "schedulers = [\n",
        "    f\"gcloud scheduler jobs create http nhanh-etl-bronze-schedule --location={REGION_SCHEDULER} --schedule='*/30 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--entity\\\",\\\"all\\\",\\\"--incremental\\\"]}}'\",\n",
        "    f\"gcloud scheduler jobs create http nhanh-etl-silver-schedule --location={REGION_SCHEDULER} --schedule='15,45 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-silver-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--entity\\\",\\\"all\\\"]}}'\",\n",
        "    f\"gcloud scheduler jobs create http nhanh-etl-gold-schedule --location={REGION_SCHEDULER} --schedule='30 * * * *' --uri='https://{REGION_SCHEDULER}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/nhanh-etl-gold-job:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID} --message-body='{{\\\"args\\\":[\\\"--aggregate\\\",\\\"all\\\"]}}'\",\n",
        "    f\"gcloud scheduler jobs create http etl-api-bigquery-nhanh-bills-daily-sync-schedule --location={REGION_SCHEDULER} --schedule='0 1 * * *' --uri='https://{REGION_RUN}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/{PROJECT_ID}/jobs/etl-api-bigquery-nhanh-bills-daily-sync:run' --http-method=POST --oauth-service-account-email={SERVICE_ACCOUNT} --time-zone='Asia/Ho_Chi_Minh' --project={PROJECT_ID}\"\n",
        "]\n",
        "\n",
        "for cmd in schedulers:\n",
        "    print(f\"Running: {cmd[:50]}...\")\n",
        "    # Allow failure if exists\n",
        "    subprocess.run(cmd + \" 2>nul || echo 'Job likely exists'\", shell=True)"
    ]
},
{
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 4. Setup Transform Pipeline (Cloud Run & Eventarc)\n",
        "Automates the `nhanh-bills-transform-job` triggered by GCS uploads."
    ]
},
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "BUCKET_NAME = PROJECT_ID\n",
        "JOB_NAME = \"nhanh-bills-transform-job\"\n",
        "SCHEDULER_NAME = \"nhanh-bills-transform-schedule\"\n",
        "TRIGGER_NAME = \"gcs-trigger-bills-transform\"\n",
        "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/nhanh-etl:latest\"\n",
        "\n",
        "# See previous cells for logic implementation (reusing similar logic here)\n",
        "# ... (Implementation logic same as drafted before)"
    ]
},
{
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 5. Setup Alerts\n",
        "Creates monitoring policies for ETL failures, data quality, and rate limits."
    ]
},
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Note: CHANNEL_ID needs to be manually set or retrieved\n",
        "CHANNEL_ID = \"projects/{PROJECT_ID}/notificationChannels/YOUR_CHANNEL_ID\" # Placeholder\n",
        "\n",
        "alerts = [\n",
        "    f\"gcloud alpha monitoring policies create --notification-channels={CHANNEL_ID} --display-name='ETL Job Failure Alert' --condition-display-name='Job execution failed' --condition-threshold-value=1 --condition-threshold-duration=0s --condition-filter='resource.type=\\\"cloud_run_job\\\" AND resource.labels.job_name=~\\\"nhanh-etl.*\\\" AND jsonPayload.status=\\\"failure\\\"' --project={PROJECT_ID}\",\n",
        "    f\"gcloud alpha monitoring policies create --notification-channels={CHANNEL_ID} --display-name='Data Quality Alert' --condition-display-name='Data quality below 95%' --condition-threshold-value=0.95 --condition-threshold-duration=300s --condition-filter='jsonPayload.quality_score < 0.95' --project={PROJECT_ID}\",\n",
        "]\n",
        "\n",
        "print(\"Warning: Create notification channels first. Skipping execution unless CHANNEL_ID is set.\")\n",
        "# for cmd in alerts: !{cmd}"
    ]
}\n"," ]\n",
"}"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Step 1: Build Image\n",
"# Note: Usually needs to be run from project root. \n",
"# Please adjust the path to 'src/features/nhanh' relative to root if needed.\n",
"# Here we assume running from notebook directory.\n",
"import os\n",
"# Go to project root (up 3 levels from src/features/nhanh/notebooks)\n",
"os.chdir(\"../../../../\")\n",
"print(\"Current working directory:\", os.getcwd())\n",
"\n",
"!gcloud builds submit --tag {IMAGE_NAME} --project={PROJECT_ID}"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Step 2: Create/Update Cloud Run Job\n",
"# Note: Using !gcloud execution.\n",
"\n",
"cmd_create_job = f\"\"\"gcloud run jobs create {JOB_NAME} \\\n",
" --image={IMAGE_NAME} \\\n",
" --region={REGION_RUN} \\\n",
" --service-account={SERVICE_ACCOUNT} \\\n",
" --memory=2Gi \\\n",
" --cpu=2 \\\n",
" --max-retries=2 \\\n",
" --task-timeout=1800 \\\n",
" --set-env-vars=\"GCP_PROJECT={PROJECT_ID},GCP_REGION={REGION_RUN},BRONZE_BUCKET={BUCKET_NAME},BRONZE_DATASET=bronze,TARGET_DATASET=nhanhVN,PARTITION_STRATEGY=month,LOG_LEVEL=INFO\" \\\n",
" --command=\"python\" \\\n",
" --args=\"src/features/nhanh/bills/transform_trigger.py\" \\\n",
" --project={PROJECT_ID}\"\"\"\n",
"\n",
"cmd_update_job = f\"\"\"gcloud run jobs update {JOB_NAME} \\\n",
" --image={IMAGE_NAME} \\\n",
" --region={REGION_RUN} \\\n",
" --service-account={SERVICE_ACCOUNT} \\\n",
" --memory=2Gi \\\n",
" --cpu=2 \\\n",
" --max-retries=2 \\\n",
" --task-timeout=1800 \\\n",
" --set-env-vars=\"GCP_PROJECT={PROJECT_ID},GCP_REGION={REGION_RUN},BRONZE_BUCKET={BUCKET_NAME},BRONZE_DATASET=bronze,TARGET_DATASET=nhanhVN,PARTITION_STRATEGY=month,LOG_LEVEL=INFO\" \\\n",
" --command=\"python\" \\\n",
" --args=\"src/features/nhanh/bills/transform_trigger.py\" \\\n",
" --project={PROJECT_ID}\"\"\"\n",
"\n",
"# Logic to check if job exists then update or create\n",
"# For notebook simplicity, we can try update, if fails, try create, or just use `gcloud run jobs create ...` with allow failure if exists\n",
"# Using python to handle logic is cleaner\n",
"\n",
"import subprocess\n",
"\n",
"check_job = subprocess.run(f\"gcloud run jobs describe {JOB_NAME} --region={REGION_RUN} --project={PROJECT_ID}\", shell=True, capture_output=True)\n",
"\n",
"if check_job.returncode == 0:\n",
"    print(\"Job exists, updating...\")\n",
"    !{cmd_update_job}\n",
"else:\n",
"    print(\"Job does not exist, creating...\")\n",
"    !{cmd_create_job}"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Step 3: Eventarc Trigger\n",
"# (Assuming APIs and permissions are enabled/granted as per one-time setup)\n",
"\n",
"# Delete if exists to recreate (simplify)\n",
"!gcloud eventarc triggers delete {TRIGGER_NAME} --location={REGION_RUN} --project={PROJECT_ID} --quiet || echo \"Trigger not found\"\n",
"\n",
"!gcloud eventarc triggers create {TRIGGER_NAME} --location={REGION_RUN} --destination-run-job={JOB_NAME} --destination-run-job-region={REGION_RUN} --destination-run-job-task-timeout=1800 --event-filters=\"type=google.cloud.storage.object.v1.finalized\" --event-filters=\"bucket={BUCKET_NAME}\" --event-filters=\"objectNamePrefix=nhanh/\" --event-filters=\"objectNameSuffix=.parquet\" --service-account={SERVICE_ACCOUNT} --project={PROJECT_ID}"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"nbformat": 4,
"nbformat_minor": 4
}